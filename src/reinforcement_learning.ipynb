{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## Our model\n",
    "We will initially be using a MLP Regression model set up with the default parameters from scikit-learn's MLP Regression model since it seems like a solid place to start. After some trial and error, a second Dense layer for the model was added to hopefully capture a bit more complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "\tkeras.layers.Dense(64, activation='relu'),\n",
    "\tkeras.layers.Dense(64, activation='relu'),\n",
    "\tkeras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the exploration rate\n",
    "The exploration rate is defined as the chance of randomly making a move instead of relying on the prediction of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_games = 1000\n",
    "exploration_rates = np.linspace(1, 0, num_games)**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.set_xlabel('Number of games played')\n",
    "ax.set_ylabel('Exploration rate')\n",
    "ax.plot(np.linspace(1, 1000, num_games), exploration_rates)\n",
    "ax.set_title('Exploration rate as the number of games played increases')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The model will be trained by playing against itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from chessbot.chessbot import ChessBot\n",
    "\n",
    "def play_game_and_learn(model, exploration_rate=0.0, should_visualise=False):\n",
    "\twhite = ChessBot(model, chess.WHITE, exploration_rate)\n",
    "\tblack = ChessBot(model, chess.BLACK, exploration_rate)\n",
    "\n",
    "\tboard = chess.Board()\n",
    "\n",
    "\tif should_visualise:\n",
    "\t\tdisplay(board)\n",
    "\n",
    "\twhile not board.is_game_over(claim_draw=True):\n",
    "\t\tboard.push(black.move(board) if board.turn == chess.BLACK else white.move(board))\n",
    "\n",
    "\t\tif should_visualise:\n",
    "\t\t\tclear_output(wait=True)\n",
    "\t\t\tdisplay(board)\n",
    "\n",
    "\tresult = board.outcome(claim_draw=True).result()\n",
    "\tX = np.concatenate([black.moves_made, white.moves_made])\n",
    "\t\n",
    "\tif result == '1-0':\n",
    "\t\ty = np.ones(len(X))\n",
    "\telif result == '0-1':\n",
    "\t\ty = np.zeros(len(X))\n",
    "\telse:\n",
    "\t\ty = np.concatenate([np.full(len(black.moves_made), 0.75), np.full(len(white.moves_made), 0.25)])\n",
    "\t\t\n",
    "\tmodel.fit(X, y)\n",
    "\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "results = []\n",
    "white_wins = 0\n",
    "black_wins = 0\n",
    "draws = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "\tclear_output(wait=True)\n",
    "\tif i > 0:\n",
    "\t\tprint(f\"The last game's result: {results[-1]}\")\n",
    "\tprint(f'White wins: {white_wins}, Black wins: {black_wins}, Draws: {draws}')\n",
    "\tprint(f'Game {i + 1}/{num_games} (exploration rate: {exploration_rates[i]:.2f})')\n",
    "\n",
    "\tresult = play_game_and_learn(model, exploration_rate=exploration_rates[i])\n",
    "\tresults.append(result)\n",
    "\n",
    "\tmatch result:\n",
    "\t\tcase '1-0':\n",
    "\t\t\twhite_wins += 1\n",
    "\t\tcase '0-1':\n",
    "\t\t\tblack_wins += 1\n",
    "\t\tcase '1/2-1/2':\n",
    "\t\t\tdraws += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'White wins: {white_wins}, Black wins: {black_wins}, Draws: {draws}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not watch a game after all that training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = play_game_and_learn(model, should_visualise=True)\n",
    "print(f'Game result: {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
