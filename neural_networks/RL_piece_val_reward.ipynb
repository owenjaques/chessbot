{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\n",
        "Regression model using keras. Currently setup in a very default way with little optimizations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYMalYMGzwoT"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model = keras.Sequential([\n",
        "\tkeras.Input(shape=(166,)),\n",
        "\tkeras.layers.Dense(128, kernel_regularizer=keras.regularizers.L1(0.01) ,activation='relu'),\n",
        "\tkeras.layers.Dense(1,)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss='mse',\n",
        "    metrics=[keras.metrics.MeanAbsoluteError()]\n",
        ")\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='loss', restore_best_weights=True, patience=3, verbose=1)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, min_lr=0.00001, verbose=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tMYyHWtfzwog"
      },
      "source": [
        "## Setting the exploration rate\n",
        "The exploration rate is defined as the chance of randomly making a move instead of relying on the prediction of the model. The pause is for when running the train function as it will clear the graph too fast to see if its right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "a2rIE3D3zwon",
        "outputId": "ac02d7e8-89b5-4883-ab93-0e5c71b7aa96"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def get_exp_rate(num_games, start = 1, stop = 0.5, scale = 0.5):\n",
        "    exploration_rates = np.linspace(start, stop, num_games)**scale\n",
        "    fig, ax = plt.subplots(figsize=(12, 5))\n",
        "    ax.set_xlabel('Number of games played')\n",
        "    ax.set_ylabel('Exploration rate')\n",
        "    ax.plot(np.linspace(1, num_games, num_games), exploration_rates)\n",
        "    ax.set_title('Exploration rate as the number of games played increases')\n",
        "    plt.show()\n",
        "    time.sleep(3)\n",
        "    return exploration_rates"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8SR_Dpj1zwox"
      },
      "source": [
        "## Model interactions\n",
        "These are the functions that actually play games and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8P-Jlb25zwo1"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from chessbot.chessbot import ChessBot\n",
        "\n",
        "def get_piece_values(board):\n",
        "\treward = 0\n",
        "\tpiece_map = board.piece_map()\n",
        "\tfor _, piece in piece_map.items():\n",
        "\t\tcolor = 1 if piece.color else -1\n",
        "\n",
        "\n",
        "\t\tif piece.piece_type == chess.ROOK:\n",
        "\t\t\treward += 5*color\n",
        "\t\telif piece.piece_type == chess.BISHOP or piece.piece_type == chess.KNIGHT:\n",
        "\t\t\treward += 3*color\n",
        "\t\telif piece.piece_type == chess.QUEEN:\n",
        "\t\t\treward += 8*color\n",
        "\t\telif piece.piece_type == chess.PAWN:\n",
        "\t\t\treward += 1*color\n",
        "\t\tr = reward/38 if (abs(reward/38))<1 else (reward/abs(reward))\n",
        "\treturn r\n",
        "\n",
        "\n",
        "\n",
        "def play_game_and_learn(model, games_per_train = 1, exploration_rate=0.0, should_visualise=False):\n",
        "\tresults = np.array([0,0,0])\n",
        "\tXs = None\n",
        "\tys = None\n",
        "\tfor i in range(games_per_train):\n",
        "\t\twhite = ChessBot(model, chess.WHITE, exploration_rate)\n",
        "\t\tblack = ChessBot(model, chess.BLACK, exploration_rate)\n",
        "\t\tboard = chess.Board()\n",
        "\t\tif should_visualise:\n",
        "\t\t\tdisplay(board)\n",
        "\n",
        "\t\treward = []\n",
        "\t\twhile not board.is_game_over(claim_draw=True):\n",
        "\t\t\tboard.push(black.move(board) if board.turn == chess.BLACK else white.move(board))\n",
        "\t\t\treward.append(get_piece_values(board))\n",
        "\t\t\tif should_visualise:\n",
        "\t\t\t\tclear_output(wait=True)\n",
        "\t\t\t\tdisplay(board)\n",
        "\t\t\t\ttime.sleep(0.1)\n",
        "\n",
        "\t\tresult = board.outcome(claim_draw=True).result()\n",
        "\n",
        "\t\t# If the game was not a draw train the model\n",
        "\t\t#if result == '1/2-1/2':\n",
        "\t\t\t#return result, None, None\n",
        "\t\t\n",
        "\t\tX = np.empty((len(white.moves_made) + len(black.moves_made), len(white.moves_made[0])))\n",
        "\t\ty = np.array(reward)\n",
        "\n",
        "\t\t# Blend the moves into a single array with alternating elements\n",
        "\t\tX[::2] = white.moves_made\n",
        "\t\tX[1::2] = black.moves_made\n",
        "\n",
        "\t\t# Set the label for the last move to 1, representing a winning move, then\n",
        "\t\t# discount the rest as they led to a win but should not be rewarded as heavily\n",
        "\t\tdiscount_factor = 0.98\n",
        "\t\t#y_reversed_indices = np.linspace(len(y) - 1, 0, num=len(y))\n",
        "\t\t#y = 1 * discount_factor**y_reversed_indices\n",
        "\n",
        "\t\tif result != '1/2-1/2':\n",
        "\t\t\t# If black won flip the labels since our model evaluates white's position\n",
        "\t\t\tfor i in range(5,0,-1):\n",
        "\t\t\t\tif result == '1-0':\n",
        "\t\t\t\t\ty[len(y)-i] = 1*discount_factor**(i-1)\n",
        "\t\t\t\telif result == '0-1':\n",
        "\t\t\t\t\ty[len(y)-i] = -1*discount_factor**(i-1)\n",
        "\n",
        "\t\t# Scale the labels to be between 0 and 1 instead of -1 and 1\n",
        "\t\t\n",
        "\t\ty = (y + 1) / 2\n",
        "\n",
        "\t\tif X is not None and y is not None:\n",
        "\t\t\tif Xs is None:\n",
        "\t\t\t\tXs = X\n",
        "\t\t\t\tys = y\n",
        "\t\t\telse:\n",
        "\t\t\t\tXs = np.concatenate((Xs, X))\n",
        "\t\t\t\tys = np.concatenate((ys, y))\n",
        "\t\t\n",
        "\t\tif result == '1-0':\n",
        "\t\t\tresults[0] += 1\n",
        "\t\tif result == '0-1':\n",
        "\t\t\tresults[1] += 1\n",
        "\t\tif result == '1/2-1/2':\n",
        "\t\t\tresults[2] += 1\n",
        "\tif not np.isnan(Xs).any():\n",
        "\t\tmodel.fit(Xs, ys, epochs= 10, shuffle=True, batch_size=256, callbacks=[early_stopping, reduce_lr])\n",
        "\telse:\n",
        "\t\tprint(\"Not training because of NAN data\")\n",
        "\n",
        "\treturn results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n",
        "The model will be trained by playing against itself. The train function allows control over most training parameter all in one place. The last cell loops train to with simple param varrying to provide more controll to how the model is trained. This also allow you to pause in the middle of training easier and with less lost data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xYo3-V42QAhM"
      },
      "outputs": [],
      "source": [
        "# Initialize objects to track game results (only run this cell when you want to reset results)\n",
        "def train(save_directory, start, stop, scale, num_games = 5000, paused_on = 0, games_per_fit = 1):\n",
        "    exploration_rates = get_exp_rate(num_games, start, stop, scale)\n",
        "    X_all = None\n",
        "    y_all = None\n",
        "    results = np.array([0,0,0])\n",
        "\n",
        "    for i in range(paused_on, num_games, games_per_fit):\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        print(f'White wins: {results[0]}, Black wins: {results[1]}, Draws: {results[2]}')\n",
        "        print(f'Game {i}/{num_games} (exploration rate: {exploration_rates[i]:.2f})')\n",
        "\n",
        "        result= play_game_and_learn(model, games_per_fit, exploration_rate=exploration_rates[i])\n",
        "        results += result\n",
        "        '''\n",
        "        if X is not None and y is not None:\n",
        "            if X_all is None:\n",
        "                X_all = X\n",
        "                y_all = y\n",
        "            else:\n",
        "                X_all = np.concatenate((X_all, X))\n",
        "                y_all = np.concatenate((y_all, y))\n",
        "        '''\n",
        "        \n",
        "\n",
        "    model.save(f'{save_directory}{i+games_per_fit}_games_model')\n",
        "    #np.savez_compressed(f'{save_directory}{i+games_per_fit}_games_data.npz', X=X_all, y=y_all)\n",
        "    print(f'###Final tally###\\nWhite wins: {results[0]}, Black wins: {results[1]}, Draws: {results[2]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_xQn9DMzwpC",
        "outputId": "9f7d4004-18e7-405f-f2f8-fb4c82fa3713"
      },
      "outputs": [],
      "source": [
        "#Must change this up before running\n",
        "for i in range(1):\n",
        "    discount = 0.9\n",
        "    num_games = 1000*(i+1)\n",
        "    paused_on = 1000*i\n",
        "    scale = 2\n",
        "    start = 1*discount**i\n",
        "    stop = 0.5*discount**i\n",
        "    train('chessbot_weights/', start, stop, scale, num_games, paused_on)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoQJZom5zwpG"
      },
      "source": [
        "## Why not watch a game after all that training?\n",
        "Plays a game with the current model to watch how its doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "_CEFav6GzwpL",
        "outputId": "58b61807-78de-405d-c2cd-a96f68399e44"
      },
      "outputs": [],
      "source": [
        "result = play_game_and_learn(model, should_visualise=True)\n",
        "print(f'Game result: {result[0]}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
