{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxs4B2ekzwoG"
      },
      "source": [
        "# Neural Network Models\n",
        "Using the labelled Lichess data, this notebook will attempt to find a neural network which can fit that labelled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twp1klKKTEj3"
      },
      "source": [
        "## Pre-requisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5dmXeDrTEj9"
      },
      "source": [
        "### If running on Google Collab\n",
        "If not running on Google collab do not run these next two cells!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8gnO_uq3ZBe",
        "outputId": "714b82a8-cf97-4918-a057-7eb873cd9de7"
      },
      "outputs": [],
      "source": [
        "# Install the only dependency not available from collab directly\n",
        "!pip install chess\n",
        "\n",
        "# Get imported files from repo\n",
        "!git clone -b lichess-neural-networks https://github.com/owenjaques/chessbot.git\n",
        "!mv chessbot chessbot-repo\n",
        "!mv chessbot-repo/neural_networks/chessbot .\n",
        "!rm chessbot-repo -r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJoqUIfm5Ory",
        "outputId": "bb14b6b2-1e13-40f2-db45-dee667f7590b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "working_directory = '/content/gdrive/MyDrive/chessbot_weights/'\n",
        "data_directory = working_directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lTAAinwzOS8"
      },
      "source": [
        "### If not running on Google Collab\n",
        "Set the weights directory variable to wherever you would like data saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_EV6byQzOS_"
      },
      "outputs": [],
      "source": [
        "!mkdir -p bin\n",
        "working_directory = './bin'\n",
        "data_directory = '../pre_processing/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut9n2VzSTEkg"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l0poAsDzOTc"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "import chess.pgn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import math\n",
        "import threading\n",
        "from chessbot import modelinput"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjATcT3mTEkr"
      },
      "source": [
        "### Data generator For Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9c5M8uKIzr5"
      },
      "outputs": [],
      "source": [
        "# There is a wierd bug here where more than one worker will cause the input data to get slightly\n",
        "# messed up, however, using 1 worker seems to be infinitely faster than running this in a loop,\n",
        "# which is super odd, but I'm not complaining.\n",
        "def worker(input, output, input_type, start_index, num):\n",
        "    input_generator = modelinput.ModelInput(input_type)\n",
        "    \n",
        "    for i in range(num):\n",
        "        output[start_index + i] = input_generator.get_input_from_fen(input[start_index + i])\n",
        "\n",
        "    print(f'\\rThread {start_index // num} completed.', end='')\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, data_file, input_type, batch_size=32, pre_process=True, save_file=None, workers=1):\n",
        "        self.batch_size = batch_size\n",
        "        data = np.load(data_file, allow_pickle=True)\n",
        "        self.y = data['y']\n",
        "        self.n = len(self.y)\n",
        "\n",
        "        if pre_process:\n",
        "            self.X = np.full((len(data['X']), modelinput.ModelInput(input_type).input_length()), -1.0)\n",
        "            step = self.n // workers\n",
        "\n",
        "            threads = []\n",
        "            for i in range(0, self.n, step):\n",
        "                threads.append(threading.Thread(target=worker, args=(data['X'], self.X, input_type, i, step)))\n",
        "                threads[-1].start()\n",
        "\n",
        "            for i in range(workers):\n",
        "                threads[i].join()\n",
        "            \n",
        "            if save_file != None:\n",
        "                np.savez_compressed(save_file, X=self.X, y=self.y)\n",
        "        else:\n",
        "            self.X = data['X']\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.X) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Returns one batch of data\n",
        "        low = idx * self.batch_size\n",
        "        high = np.minimum(low + self.batch_size, self.n)\n",
        "        return self.X[low:high], self.y[low:high]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivk-Ec-jTEk7"
      },
      "source": [
        "## Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-tv7B_6nJgd"
      },
      "source": [
        "### Pre-process the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwfCqFHkTElA",
        "outputId": "772514c8-93fc-4585-c773-5f70423660fa"
      },
      "outputs": [],
      "source": [
        "# Data generators for training the model\n",
        "training_data = DataGenerator(data_directory + '/training_set.npz', 'positions', save_file=data_directory + '/positions_training_set.npz')\n",
        "validation_data = DataGenerator(data_directory + '/validation_set.npz', 'positions', save_file=data_directory + '/positions_validation_set.npz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsljp8gjy7tM"
      },
      "source": [
        "#### Sanity check\n",
        "Here we check that the training data was transformed correctly. X and y should both be in the range [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "LEcgyjfHwAxh",
        "outputId": "275465b8-66e5-4f18-af8f-f19d5d4f1ea7"
      },
      "outputs": [],
      "source": [
        "print(f'Training data: n={training_data.n} len(X)={len(training_data.X)}')\n",
        "print(f'Validation data: n={validation_data.n} len(X)={len(validation_data.X)}')\n",
        "\n",
        "_, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
        "\n",
        "ax[0, 0].hist(training_data.X.flatten(), bins=100)\n",
        "ax[0, 0].set_title(f'Training Dataset X')\n",
        "ax[0, 0].set_xlabel('label (y)')\n",
        "ax[0, 0].set_ylabel('no. of occurences in dataset')\n",
        "\n",
        "ax[0, 1].hist(training_data.y, bins=100)\n",
        "ax[0, 1].set_title(f'Training Dataset y')\n",
        "ax[0, 1].set_xlabel('label (y)')\n",
        "ax[0, 1].set_ylabel('no. of occurences in dataset')\n",
        "\n",
        "ax[1, 0].hist(validation_data.X.flatten(), bins=100)\n",
        "ax[1, 0].set_title(f'Validation Dataset X')\n",
        "ax[1, 0].set_xlabel('label (y)')\n",
        "ax[1, 0].set_ylabel('no. of occurences in dataset')\n",
        "\n",
        "ax[1, 1].hist(validation_data.y, bins=100)\n",
        "ax[1, 1].set_title(f'Validation Dataset y')\n",
        "ax[1, 1].set_xlabel('label (y)')\n",
        "ax[1, 1].set_ylabel('no. of occurences in dataset')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "rZCBEiPrF00F",
        "outputId": "87827ac3-b778-4592-f948-2c58874dc83a"
      },
      "outputs": [],
      "source": [
        "# Reduce the 0.5 bin by approximitly 50%\n",
        "for i in range(training_data.n):\n",
        "    if 0.475 < training_data.y[i] < 0.525:\n",
        "        if np.random.rand() < 0.6:\n",
        "            index = math.floor((np.random.rand() * training_data.n))\n",
        "            training_data.y[i] = training_data.y[index]\n",
        "            training_data.X[i] = training_data.X[index]\n",
        "\n",
        "for i in range(validation_data.n):\n",
        "    if 0.475 < validation_data.y[i] < 0.525:\n",
        "        if np.random.rand() < 0.6:\n",
        "            index = math.floor((np.random.rand() * validation_data.n))\n",
        "            validation_data.y[i] = validation_data.y[index]\n",
        "            validation_data.X[i] = validation_data.X[index]\n",
        "\n",
        "_, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
        "\n",
        "ax[0, 0].hist(training_data.X.flatten(), bins=100)\n",
        "ax[0, 0].set_title(f'Training Dataset X')\n",
        "ax[0, 0].set_xlabel('label (y)')\n",
        "ax[0, 0].set_ylabel('no. of occurences in dataset')\n",
        "\n",
        "ax[0, 1].hist(training_data.y, bins=100)\n",
        "ax[0, 1].set_title(f'Training Dataset y')\n",
        "ax[0, 1].set_xlabel('label (y)')\n",
        "ax[0, 1].set_ylabel('no. of occurences in dataset')\n",
        "\n",
        "ax[1, 0].hist(validation_data.X.flatten(), bins=100)\n",
        "ax[1, 0].set_title(f'Validation Dataset X')\n",
        "ax[1, 0].set_xlabel('label (y)')\n",
        "ax[1, 0].set_ylabel('no. of occurences in dataset')\n",
        "\n",
        "ax[1, 1].hist(validation_data.y, bins=100)\n",
        "ax[1, 1].set_title(f'Validation Dataset y')\n",
        "ax[1, 1].set_xlabel('label (y)')\n",
        "ax[1, 1].set_ylabel('no. of occurences in dataset')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czVZrHOk6Qqh"
      },
      "source": [
        "### The model we will be using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4x0OtsVnMMr"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "\tkeras.layers.Dense(1024, activation='relu'),\n",
        "\tkeras.layers.Dense(1024, activation='relu'),\n",
        "\tkeras.layers.Dense(1024, activation='relu'),\n",
        "\tkeras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "\tmetrics=[keras.metrics.MeanAbsoluteError()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SR_Dpj1zwox"
      },
      "source": [
        "## Training a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_xQn9DMzwpC",
        "outputId": "bc56018e-7c09-4674-b15b-79228f09b463"
      },
      "outputs": [],
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    restore_best_weights=True,\n",
        "    patience=5,\n",
        "    verbose=1)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=1,\n",
        "    min_lr=0.000000000001,\n",
        "    verbose=1)\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    f'{working_directory}/model',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True)\n",
        "\n",
        "tensorboard = keras.callbacks.TensorBoard(\n",
        "    log_dir=f'{working_directory}/logs',\n",
        "    write_graph=True,\n",
        "    write_images=True,\n",
        "    histogram_freq=1)\n",
        "\n",
        "model.fit(\n",
        "    training_data,\n",
        "    epochs=128,\n",
        "    validation_data=validation_data,\n",
        "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhP-NlZFTElN"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMqv5oliZ-Zr"
      },
      "source": [
        "### Optionally load a previous model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4WOshr4vnBH"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(f'{working_directory}/model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5FptVDqf79"
      },
      "source": [
        "### Histograms and predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "3xb9RhKoqfA9",
        "outputId": "0b374aab-633b-4426-e845-506f3d2b5201"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "testing_data = DataGenerator(data_directory + '/test_set.npz', 'positions', save_file=data_directory + '/positions_test_set.npz', batch_size=1)\n",
        "evaluation = model.evaluate(testing_data)\n",
        "predictions = model.predict(testing_data)\n",
        "\n",
        "_, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "ax[0].hist(predictions, bins=50)\n",
        "ax[0].set_title(f'Predicted labels')\n",
        "ax[0].set_xlabel('label (y)')\n",
        "ax[0].set_ylabel('no. of occurences in dataset')\n",
        "ax[1].hist(testing_data.y, bins=50)\n",
        "ax[1].set_title(f'Actual labels')\n",
        "ax[1].set_xlabel('label (y)')\n",
        "ax[1].set_ylabel('no. of occurences in dataset')\n",
        "plt.show()\n",
        "\n",
        "# Bin the data into continuous intervals, then plot a confusion matrix\n",
        "predictions_binned = np.digitize(predictions, bins=np.linspace(0, 1, 10))\n",
        "y_binned = np.digitize(testing_data.y, bins=np.linspace(0, 1, 10))\n",
        "cm = confusion_matrix(y_binned, predictions_binned)\n",
        "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
        "ax.set(xlabel='Predicted label', ylabel='True label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUvGOIrU_bjf"
      },
      "source": [
        "## Why not play a game after all that training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "0UQyIKnj_h9E",
        "outputId": "8a6bd879-1b76-4093-8e78-ef1227b5e141"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "from chessbot.chessbot import ChessBot\n",
        "\n",
        "def play_game(model, exploration_rate=0.0, should_visualise=False):\n",
        "\twhite = ChessBot(model, chess.WHITE, exploration_rate)\n",
        "\tboard = chess.Board()\n",
        "\n",
        "\tif should_visualise:\n",
        "\t\tdisplay(board)\n",
        "\n",
        "\twhile not board.is_game_over(claim_draw=True):\n",
        "\t\tboard.push(chess.Move.from_uci(input()) if board.turn == chess.BLACK else white.move(board))\n",
        "\n",
        "\t\tif should_visualise:\n",
        "\t\t\ttime.sleep(1)\n",
        "\t\t\tclear_output(wait=True)\n",
        "\t\t\tdisplay(board)\n",
        "\n",
        "\treturn board.outcome(claim_draw=True).result()\n",
        " \n",
        "play_game(model, should_visualise=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
