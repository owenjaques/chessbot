{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxs4B2ekzwoG"
      },
      "source": [
        "# Reinforcement Learning\n",
        "## Some pre-requisites if running on Google Collab\n",
        "If not running on Google collab do not run these next two cells!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8gnO_uq3ZBe",
        "outputId": "f5a50a0e-fff7-4c50-cc18-4f7a7aa02a80"
      },
      "outputs": [],
      "source": [
        "# Install the only dependency not available from collab directly\n",
        "!pip install chess\n",
        "\n",
        "# Get imported files from repo\n",
        "!git clone -b rl-setup https://github.com/owenjaques/chessbot.git\n",
        "!mv chessbot chessbot-repo\n",
        "!mv chessbot-repo/src/chessbot .\n",
        "!rm chessbot-repo -r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJoqUIfm5Ory",
        "outputId": "b94a1bed-b709-4c53-8507-758495b0a9ec"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "weights_directory = '/content/gdrive/MyDrive/chessbot_weights/'\n",
        "print(f'Saving weights to {weights_directory}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## If not running on Google Collab\n",
        "Set the weights directory variable to wherever you would like data saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_directory = 'your directory here'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4NoDAvLu3WvE"
      },
      "source": [
        "## Our model\n",
        "In this section initialize your model. A fairly standard neural network is currently set up there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYMalYMGzwoT"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "\tkeras.layers.Dense(64, activation='relu'),\n",
        "\tkeras.layers.Dense(64, activation='relu'),\n",
        "\tkeras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss='mse'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMYyHWtfzwog"
      },
      "source": [
        "## Setting the exploration rate\n",
        "The exploration rate is defined as the chance of randomly making a move instead of relying on the prediction of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "a2rIE3D3zwon",
        "outputId": "ac02d7e8-89b5-4883-ab93-0e5c71b7aa96"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "num_games = 10000\n",
        "exploration_rates = np.linspace(1, 0, num_games)**2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "ax.set_xlabel('Number of games played')\n",
        "ax.set_ylabel('Exploration rate')\n",
        "ax.plot(np.linspace(1, num_games, num_games), exploration_rates)\n",
        "ax.set_title('Exploration rate as the number of games played increases')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SR_Dpj1zwox"
      },
      "source": [
        "## Training the model\n",
        "The model will be trained by playing against itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P-Jlb25zwo1"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from chessbot.chessbot import ChessBot\n",
        "\n",
        "def play_game_and_learn(model, exploration_rate=0.0, should_visualise=False):\n",
        "\twhite = ChessBot(model, chess.WHITE, exploration_rate)\n",
        "\tblack = ChessBot(model, chess.BLACK, exploration_rate)\n",
        "\n",
        "\tboard = chess.Board()\n",
        "\n",
        "\tif should_visualise:\n",
        "\t\tdisplay(board)\n",
        "\n",
        "\twhile not board.is_game_over(claim_draw=True):\n",
        "\t\tboard.push(black.move(board) if board.turn == chess.BLACK else white.move(board))\n",
        "\n",
        "\t\tif should_visualise:\n",
        "\t\t\tclear_output(wait=True)\n",
        "\t\t\tdisplay(board)\n",
        "\t\t\ttime.sleep(0.5)\n",
        "\n",
        "\tresult = board.outcome(claim_draw=True).result()\n",
        "\n",
        "\t# If the game was not a draw train the model\n",
        "\tif result == '1/2-1/2':\n",
        "\t\treturn result, None, None\n",
        "\t\n",
        "\tX = np.empty((len(white.moves_made) + len(black.moves_made), len(white.moves_made[0])))\n",
        "\ty = np.empty_like(X)\n",
        "\n",
        "\t# Blend the moves into a single array with alternating elements\n",
        "\tX[::2] = white.moves_made\n",
        "\tX[1::2] = black.moves_made\n",
        "\n",
        "\t# Set the label for the last move to 1, representing a winning move, then\n",
        "\t# discount the rest as they led to a win but should not be rewarded as heavily\n",
        "\tdiscount_factor = 0.95\n",
        "\ty_reversed_indices = np.linspace(len(y) - 1, 0, num=len(y))\n",
        "\ty = 1 * discount_factor**y_reversed_indices\n",
        "\n",
        "\t# If black won flip the labels since our model evaluates white's position\n",
        "\tif result == '0-1':\n",
        "\t\ty *= -1\n",
        "\n",
        "\t# Scale the labels to be between 0 and 1 instead of -1 and 1\n",
        "\ty = (y + 1) / 2\n",
        "\t\t\n",
        "\tmodel.fit(X, y)\n",
        "\n",
        "\treturn result, X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYo3-V42QAhM"
      },
      "outputs": [],
      "source": [
        "# Initialize objects to track game results (only run this cell when you want to reset results)\n",
        "results = []\n",
        "white_wins = 0\n",
        "black_wins = 0\n",
        "draws = 0\n",
        "\n",
        "X_all = None\n",
        "y_all = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXbLcDWdzwo-",
        "outputId": "f01db6f6-4e7a-4257-ee92-195810cb2f88"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "for i in range(5116, num_games):\n",
        "\tclear_output(wait=True)\n",
        "\tif i > 0:\n",
        "\t\tprint(f\"The last game's result: {results[-1]}\")\n",
        "\tprint(f'White wins: {white_wins}, Black wins: {black_wins}, Draws: {draws}')\n",
        "\tprint(f'Game {i + 1}/{num_games} (exploration rate: {exploration_rates[i]:.2f})')\n",
        "\n",
        "\tresult, X, y = play_game_and_learn(model, exploration_rate=exploration_rates[i])\n",
        "\tresults.append(result)\n",
        "\n",
        "\tif X is not None and y is not None:\n",
        "\t\tif X_all is None:\n",
        "\t\t\tX_all = X\n",
        "\t\t\ty_all = y\n",
        "\t\telse:\n",
        "\t\t\tX_all = np.concatenate((X_all, X))\n",
        "\t\t\ty_all = np.concatenate((y_all, y))\n",
        "\n",
        "\tif result == '1-0':\n",
        "\t\twhite_wins += 1\n",
        "\tif result == '0-1':\n",
        "\t\tblack_wins += 1\n",
        "\tif result == '1/2-1/2':\n",
        "\t\tdraws += 1\n",
        "\n",
        "\t# Save the weights and training data every 100 games\n",
        "\tif (i + 1) % 1000 == 0:\n",
        "\t\tmodel.save(f'{weights_directory}{i + 1}_games_model')\n",
        "\t\tnp.savez_compressed(f'{weights_directory}games_data.npz', X=X_all, y=y_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoQJZom5zwpG"
      },
      "source": [
        "## Why not watch a game after all that training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "_CEFav6GzwpL",
        "outputId": "58b61807-78de-405d-c2cd-a96f68399e44"
      },
      "outputs": [],
      "source": [
        "result = play_game_and_learn(model, should_visualise=True)\n",
        "print(f'Game result: {result}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
